{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbaspour - 610398147 - HW4 - Polynomial Regressions\n",
    "# Problem\n",
    "The prolem is to fit some polynomials (from 1 to 4 degrees) on a given dataset and see the results.\n",
    "First some methods of some practical libraries imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, dot, random\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from time import time\n",
    "from matplotlib.pyplot import legend, plot, savefig, scatter, show, title, xlabel, ylabel\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Allocations\n",
    "Many variables are defined for handling regression algorithm parameters and for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, MSERates, splitFracts, Degs, fourthRates, Regressors, trainErrors, testErrors, modelErrors, regularizedErrors, model_trErrors, model_ttErrors, regularized_trErrors, regularized_ttErrors, splitIndexer, Block = read_csv(\"Q2data.csv\"), [(4500, 0.00185), (3750, 0.0000015), (2400, 0.0000000029)], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], [1, 2, 3], [0.0000000000028, 0.0000000000028, 0.0000000000028, 0.0000000000028, 0.00000000000285, 0.0000000000028, 0.0000000000027, 0.0000000000025, 0.0000000000025], [], [], [], {}, {}, [], [], [], [], [], False\n",
    "X, y = array([x for x in df[\"X\"]]), array([y for y in df[\"Y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Calculation\n",
    "Cost function is defined here to measure Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error as Cost Function.\n",
    "def MSE(X, W, y, n):\n",
    "    return sum([(y[Index] - dot(W, X[Index])) ** 2 for Index in range(n)]) / n       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implementation\n",
    "First dataset distribution became plotted on the screen.\n",
    "For first three degrees algorithm is implemented in fully scratch way and uses gradiet descend algorithm to find optimal hyperplane for each polynomial fitting. It uses a constent rate 60 percent of dataset as trainset.\n",
    "After that, three mentioned ploynomial fittings are plotted trying to fit the dataset.\n",
    "Learning rate of each polynomial fitting is achieved by testing not to reach +inf or -inf as value of hyperplane vector \"W\" so they're extremely small and they got gathered in 'MSERates'list defiend above has the format: [(MSE Boundary, Learning Rate) for each poynomial fitting].\n",
    "For fitting 4-degree polynomial it uses prior implemented scratch form and just uses 'Ridge' when regularizing.\n",
    "Learning rates for each splitting phase are achieved by testing many so much small values similarily avoiding getting +inf or -inf.\n",
    "4-degree polyomial fitting in each splitting phase would be plotted after fitting.\n",
    "After that, four errors recieved called 'Train Error', 'Test Error', 'Regularized Based Train Error', 'Regularized Based Test Error' are plotted for comparison the model with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modeling():\n",
    "    # Plotting raw\n",
    "    scatter(X, y, c = \"black\")\n",
    "    xlabel(\"$x$\")\n",
    "    ylabel(\"$y$\")\n",
    "    show(block = Block)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42, shuffle = True)\n",
    "    n, m = X_train.shape[0], X_test.shape[0]\n",
    "    for Degree, MSERate in enumerate(MSERates):\n",
    "        Degree += 1\n",
    "        # Train features\n",
    "        Poly = PolynomialFeatures(degree = Degree)\n",
    "        polyX_train, polyX_test, W, MSE_boundary, learningRate = Poly.fit_transform(X_train.reshape(-1, 1)), Poly.fit_transform(X_test.reshape(-1, 1)), random.rand(1, Degree + 1)[0], MSERate[0], MSERate[1]\n",
    "        print(f\"\\n\\nPrimary initialized W vector is {W} while degree equals {Degree}.\\n\\nLearning...\")\n",
    "        Start = time()\n",
    "        # Checking goodness of value of MSE error function.\n",
    "        while MSE(polyX_train, W, y_train, n) > MSE_boundary:\n",
    "            W += learningRate * sum([(y_train[Index] - dot(W, polyX_train[Index])) * polyX_train[Index] for Index in range(n)]) / n\n",
    "        End = time()\n",
    "        MSE_opt, MSE_v = MSE(polyX_train, W, y_train, n), MSE(polyX_test, W, y_test, m)\n",
    "        Regressors.append(dot(Poly.fit_transform(X.reshape(-1, 1)), W))\n",
    "        trainErrors.append(MSE_opt)\n",
    "        testErrors.append(MSE_v)\n",
    "        print(f\"\\nLearning for fitting {Degree}-degree polynomial curve on 60% of shuffled dataset occured in {End - Start} UTC time unit.\\nOptimal value of MSE cost is {MSE_opt} with optimal W vector {W}\")\n",
    "\n",
    "    # Plotting learned regressors\n",
    "    for Degree, Regressor in enumerate(Regressors): \n",
    "        scatter(X, Regressor, c = \"red\")\n",
    "        scatter(X, y, c = \"black\")\n",
    "        title(f\"{Degree + 1}-degree Polynomial Fitting.\")\n",
    "        xlabel(\"$x$\")\n",
    "        ylabel(\"$y$\")\n",
    "        show(block = Block)\n",
    "\n",
    "    scatter(Degs, trainErrors, color = \"blue\")\n",
    "    scatter(Degs, testErrors, color = \"red\")\n",
    "    plot(Degs, trainErrors, linestyle = \"-\", color = \"blue\", label = \"Train Error\")\n",
    "    plot(Degs, testErrors, linestyle = \"-\", color = \"red\", label = \"Test Error\")\n",
    "    # title\n",
    "    title(\"Error Curves\")\n",
    "    # x label\n",
    "    xlabel(\"Degree of Polynomial\")\n",
    "    # y label\n",
    "    ylabel(\"Errors\")\n",
    "    legend(loc = \"best\")\n",
    "    savefig(\"Error Curves\", dpi = 300)\n",
    "    show(block = Block)\n",
    "    # Fourth degree\n",
    "    # Scratch form\n",
    "    Poly, MSE_boundary, degreePlus = PolynomialFeatures(degree = 4), 1300, 5\n",
    "    polyX = Poly.fit_transform(X.reshape(-1, 1))\n",
    "    for splitFract, learningRate in enumerate(fourthRates):\n",
    "        splitFract = (splitFract + 1) / 10\n",
    "        if splitFract <= 0.5:\n",
    "            continue\n",
    "        Percent = int(splitFract * 100)\n",
    "        print(f\"\\n\\n4-degree polynomial fitting started!\\n{Percent}% of whole dataset used as test set.\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(polyX, y, test_size = splitFract, random_state = 42, shuffle = True)\n",
    "        W, n, m = random.rand(1, degreePlus)[0], X_train.shape[0], X_test.shape[0]\n",
    "        print(f\"Primary initialized W vector is {W}.\\n\\nLearning...\")\n",
    "        Start = time()\n",
    "        # Goodness of value of MSE error function.\n",
    "        while MSE(X_train, W, y_train, n) > MSE_boundary:\n",
    "            W += learningRate * sum([(y_train[Index] - dot(W, X_train[Index])) * X_train[Index] for Index in range(n)]) / n\n",
    "            #print(W)\n",
    "        End = time()\n",
    "        model_train_error = MSE(X_train, W, y_train, n)\n",
    "        print(f\"\\nLearning for fitting 4-degree polynomial curve on {100 - Percent}% of dataset occured in {End - Start} UTC time unit.\\nOptimal value of MSE cost is {model_train_error} with optimal W vector {W}\")\n",
    "        scatter(X, dot(polyX, W), c = \"red\")\n",
    "        scatter(X, y, c = \"black\")\n",
    "        title(f\"4-degree Polynomial Fitting for {int(splitFract * 10)}th dataset split.\")\n",
    "        xlabel(\"$x$\")\n",
    "        ylabel(\"$y$\")\n",
    "        show(block = Block)\n",
    "        \n",
    "        # Regularization strength, when needed avoiding or decreasing overfitting event.\n",
    "        regularizedModel = Ridge(alpha = 1e+5).fit(X_train, y_train)\n",
    "        # Compute Mean Squared Errors and save them for comparison and self-made evaluation phase.\n",
    "        model_test_error, regularized_train_error, regularized_test_error = MSE(X_test, W, y_test, m), mean_squared_error(y_train, regularizedModel.predict(X_train)), mean_squared_error(y_test, regularizedModel.predict(X_test))\n",
    "        model_trErrors.append(model_train_error)\n",
    "        model_ttErrors.append(model_test_error)\n",
    "        regularized_trErrors.append(regularized_train_error)\n",
    "        regularized_ttErrors.append(regularized_test_error)\n",
    "        model_errs, regularized_errs = (model_train_error, model_test_error), (regularized_train_error, regularized_test_error)\n",
    "        splitIndexer.append([model_errs, regularized_errs])\n",
    "        modelErrors[abs(model_train_error - model_test_error)], regularizedErrors[abs(regularized_train_error - regularized_test_error)] = model_errs, regularized_errs\n",
    "        print(f\"Mean Squared Error for train set in non-regularized model and regularized model are {model_train_error} and {regularized_train_error}, respectively\\nwhile Mean Squared Error for test set in non-regularized model and regularized model are {model_test_error} and {regularized_test_error}, respectively.\")\n",
    "\n",
    "    model_best_errScore, regularized_best_errScore = min(list(modelErrors.keys())), min(list(regularizedErrors.keys()))\n",
    "    if model_best_errScore < regularized_best_errScore:\n",
    "        bestErrors = modelErrors[model_best_errScore]\n",
    "    else:\n",
    "        bestErrors = regularizedErrors[regularized_best_errScore]\n",
    "\n",
    "    for Index, Errors in enumerate(splitIndexer):\n",
    "        if bestErrors in Errors:\n",
    "            bestIndex = int(Index + 1)\n",
    "            break\n",
    "    print(f\"\\n\\nBest overall score (lowest distance of train error and test error) achieved from {bestIndex}th split in which the value of MSE error for train set equals {bestErrors[0]} and the value of MSE error for test set equals {bestErrors[1]}, respectively.\")\n",
    "    scatter(splitFracts, model_trErrors, color = \"blue\")\n",
    "    scatter(splitFracts, model_ttErrors, color = \"red\")\n",
    "    scatter(splitFracts, regularized_trErrors, color = \"yellow\")\n",
    "    scatter(splitFracts, regularized_ttErrors, color = \"green\")\n",
    "\n",
    "    plot(splitFracts, model_trErrors, linestyle = \"-\", color = \"blue\", label = \"Train Error\")\n",
    "    plot(splitFracts, model_ttErrors, linestyle = \"-\", color = \"red\", label = \"Test Error\")\n",
    "    plot(splitFracts, regularized_trErrors, linestyle = \"-\", color = \"yellow\", label = \"Regularized Based Train Error\")\n",
    "    plot(splitFracts, regularized_ttErrors, linestyle = \"-\", color = \"green\", label = \"Regularized Based Test Error\")\n",
    "    # title\n",
    "    title(\"4-degree Polynomial Fitting Error Curves\")\n",
    "    # x label\n",
    "    xlabel(\"Split Fraction\")\n",
    "    # y label\n",
    "    ylabel(\"Errors\")\n",
    "    legend(loc = \"best\")\n",
    "    savefig(\"Error Curves\", dpi = 300)\n",
    "    show(block = Block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main & Evaluations.\n",
    "Testing time!\n",
    "The first three polynomial fittings has good-enough MSE cost value which are 3890.2643181981093, 3749.962387615885, and 2399.9628175216317, respcetively.\n",
    "Fitted curves for splitting phases are nearly the same and seems overfitted due to high degree of target polynomial which equals 4.\n",
    "Lowest MSE value occured in 7th splitting phase in modeling with regularization with 245.5786304598505 for train set and 242.64056064296778 for test set.\n",
    "Error curves for comparison between modeling and modeling with regularization has large distances because modeling in scratch way causes high valid MSE erro while modeling with regularization causes much less MSE value due to using 'Ridge' library implemented by professional computer scientists who fine tune learning rate and lambda parameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "Main and the biggest challange was to search for learning rates satisfying MSE value boundaries as well and making optimal hyperplane vector 'W' converge."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
